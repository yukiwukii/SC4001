{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnRX6LUnqBpw"
   },
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part B, Q1 (15 marks)\n",
    "---\n",
    "\n",
    "Real world datasets often have a mix of numeric and categorical features – this dataset is one example. To build models on such data, categorical features have to be encoded or embedded.\n",
    "\n",
    "PyTorch Tabular is a library that makes it very convenient to build neural networks for tabular data. It is built on top of PyTorch Lightning, which abstracts away boilerplate model training code and makes it easy to integrate other tools, e.g. TensorBoard for experiment tracking.\n",
    "\n",
    "For questions B1 and B2, the following features should be used:   \n",
    "- **Numeric / Continuous** features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "- **Categorical** features: month, town, flat_model_type, storey_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jA67PbIY3PnH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_tabular[extra] in ./.venv/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (2.5.1)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (1.6.1)\n",
      "Requirement already satisfied: pytorch-lightning<2.5.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (2.4.0)\n",
      "Requirement already satisfied: omegaconf>=2.3.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (2.3.0)\n",
      "Requirement already satisfied: torchmetrics<1.6.0,>=0.10.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (1.5.2)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>2.2.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (2.19.0)\n",
      "Requirement already satisfied: protobuf<5.29.0,>=3.20.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (5.28.3)\n",
      "Requirement already satisfied: pytorch-tabnet==4.1 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (4.1.0)\n",
      "Requirement already satisfied: PyYAML<6.1.0,>=5.4 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (6.0.2)\n",
      "Requirement already satisfied: matplotlib>3.1 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (3.10.1)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (8.1.5)\n",
      "Requirement already satisfied: einops<0.8.0,>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (0.7.0)\n",
      "Requirement already satisfied: rich>=11.0.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (13.9.4)\n",
      "Requirement already satisfied: wandb<0.19.0,>=0.15.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (0.18.7)\n",
      "Requirement already satisfied: plotly<5.25.0,>=5.13.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (5.24.1)\n",
      "Requirement already satisfied: kaleido<0.3.0,>=0.2.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (0.2.1)\n",
      "Requirement already satisfied: captum<0.8.0,>=0.5.0 in ./.venv/lib/python3.11/site-packages (from pytorch_tabular[extra]) (0.7.0)\n",
      "Requirement already satisfied: scipy>1.4 in ./.venv/lib/python3.11/site-packages (from pytorch-tabnet==4.1->pytorch_tabular[extra]) (1.12.0)\n",
      "Requirement already satisfied: tqdm>=4.36 in ./.venv/lib/python3.11/site-packages (from pytorch-tabnet==4.1->pytorch_tabular[extra]) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib>3.1->pytorch_tabular[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./.venv/lib/python3.11/site-packages (from omegaconf>=2.3.0->pytorch_tabular[extra]) (4.9.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.1.5->pytorch_tabular[extra]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.1.5->pytorch_tabular[extra]) (2025.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.venv/lib/python3.11/site-packages (from plotly<5.25.0,>=5.13.0->pytorch_tabular[extra]) (9.0.0)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./.venv/lib/python3.11/site-packages (from pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in ./.venv/lib/python3.11/site-packages (from pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (0.14.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=11.0.0->pytorch_tabular[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=11.0.0->pytorch_tabular[extra]) (2.19.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.3.0->pytorch_tabular[extra]) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.3.0->pytorch_tabular[extra]) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (3.7)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (75.6.0)\n",
      "Requirement already satisfied: six>1.9 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.11/site-packages (from tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (3.1.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->pytorch_tabular[extra]) (3.17.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->pytorch_tabular[extra]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->pytorch_tabular[extra]) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->pytorch_tabular[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->pytorch_tabular[extra]) (1.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (4.3.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (7.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (2.22.0)\n",
      "Requirement already satisfied: setproctitle in ./.venv/lib/python3.11/site-packages (from wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (1.3.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular[extra]) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular[extra]) (9.0.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular[extra]) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular[extra]) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.11/site-packages (from ipywidgets->pytorch_tabular[extra]) (3.0.13)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (3.11.13)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (4.0.12)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (3.0.50)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.6.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->pytorch_tabular[extra]) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard!=2.5.0,>2.2.0->pytorch_tabular[extra]) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<2.5.0,>=2.0.0->pytorch_tabular[extra]) (1.18.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb<0.19.0,>=0.15.0->pytorch_tabular[extra]) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->pytorch_tabular[extra]) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"pytorch_tabular[extra]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1\n",
      "Uninstalling torch-2.5.1:\n",
      "  Successfully uninstalled torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall \"torch\" -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.5.1\n",
      "  Using cached torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch==2.5.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch==2.5.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch==2.5.1) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch==2.5.1) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
      "Using cached torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "Installing collected packages: torch\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jr6P3U7w3NVl"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (\n",
    "    DataConfig,\n",
    "    OptimizerConfig,\n",
    "    TrainerConfig,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGyEWcVlqKTz"
   },
   "source": [
    "> Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data, and year 2021 as test data (validation set is not required).\n",
    "**Do not** use data from year 2022 and year 2023.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hoCPcOWupw5Y"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df_train = df[df['year'] <= 2020]\n",
    "df_test = df[df['year'] == 2021] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sebMgSuzqPe7"
   },
   "source": [
    "> Refer to the documentation of **PyTorch Tabular** and perform the following tasks: https://pytorch-tabular.readthedocs.io/en/latest/#usage\n",
    "- Use **[DataConfig](https://pytorch-tabular.readthedocs.io/en/latest/data/)** to define the target variable, as well as the names of the continuous and categorical variables.\n",
    "- Use **[TrainerConfig](https://pytorch-tabular.readthedocs.io/en/latest/training/)** to automatically tune the learning rate. Set batch_size to be 1024 and set max_epoch as 50.\n",
    "- Use **[CategoryEmbeddingModelConfig](https://pytorch-tabular.readthedocs.io/en/latest/models/#category-embedding-model)** to create a feedforward neural network with 1 hidden layer containing 50 neurons.\n",
    "- Use **[OptimizerConfig](https://pytorch-tabular.readthedocs.io/en/latest/optimizer/)** to choose Adam optimiser. There is no need to set the learning rate (since it will be tuned automatically) nor scheduler.\n",
    "- Use **[TabularModel](https://pytorch-tabular.readthedocs.io/en/latest/tabular_model/)** to initialise the model and put all the configs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZZWAYdNhqPzh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:24</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">522</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:24\u001b[0m,\u001b[1;36m522\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_config = DataConfig(\n",
    "    target=[\"resale_price\"],\n",
    "    continuous_cols=[\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\",\n",
    "                     \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"],\n",
    "    categorical_cols=[\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(batch_size=1024, max_epochs=50, accelerator=\"auto\", auto_lr_find=True)\n",
    "model_config = CategoryEmbeddingModelConfig(task=\"regression\", layers='50', seed=10, metrics=['mean_squared_error','r2_score'])\n",
    "optimizer_config = OptimizerConfig(optimizer='Adam')\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UXPKq0qWQG"
   },
   "source": [
    "> Report the test RMSE error and the test R2 value that you obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zmE9Bc7Nqadi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">259</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">548</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the DataLoaders                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:27\u001b[0m,\u001b[1;36m259\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m548\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the DataLoaders                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">286</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_datamodul<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:522</span><span style=\"font-weight: bold\">}</span> - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:27\u001b[0m,\u001b[1;36m286\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_datamodul\u001b[1;92me:522\u001b[0m\u001b[1m}\u001b[0m - INFO - Setting up the datamodule for          \n",
       "regression task                                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">357</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">599</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Model: CategoryEmbeddingModel \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:27\u001b[0m,\u001b[1;36m357\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m599\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Model: CategoryEmbeddingModel \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">380</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:27\u001b[0m,\u001b[1;36m380\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:27</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">417</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">656</span><span style=\"font-weight: bold\">}</span> - INFO - Auto LR Find Started                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:27\u001b[0m,\u001b[1;36m417\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m656\u001b[0m\u001b[1m}\u001b[0m - INFO - Auto LR Find Started                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/saved_models exists and is not empty.\n",
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a776bdcf6254d0f964927716875d234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.5754399373371567\n",
      "Restoring states from the checkpoint path at /Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.lr_find_747fb6cf-3461-4461-8cb0-d991af86e23f.ckpt\n",
      "Restored all states from the checkpoint at /Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.lr_find_747fb6cf-3461-4461-8cb0-d991af86e23f.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">293</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">669</span><span style=\"font-weight: bold\">}</span> - INFO - Suggested LR: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5754399373371567</span>. For plot  \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:30\u001b[0m,\u001b[1;36m293\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m669\u001b[0m\u001b[1m}\u001b[0m - INFO - Suggested LR: \u001b[1;36m0.5754399373371567\u001b[0m. For plot  \n",
       "and detailed analysis, use `find_learning_rate` method.                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:30</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">295</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">678</span><span style=\"font-weight: bold\">}</span> - INFO - Training Started                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:30\u001b[0m,\u001b[1;36m295\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m678\u001b[0m\u001b[1m}\u001b[0m - INFO - Training Started                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                      </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ _embedding_layer │ Embedding1dLayer          │  1.6 K │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ head             │ LinearHead                │     51 │ train │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss             │ MSELoss                   │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ _backbone        │ CategoryEmbeddingBackbone │  3.0 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ _embedding_layer │ Embedding1dLayer          │  1.6 K │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ head             │ LinearHead                │     51 │ train │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss             │ MSELoss                   │      0 │ train │\n",
       "└───┴──────────────────┴───────────────────────────┴────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 4.6 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 4.6 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 16                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 4.6 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 4.6 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 16                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cdaaff955f4a3695141311b2065454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">012</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">689</span><span style=\"font-weight: bold\">}</span> - INFO - Training the model completed                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:46\u001b[0m,\u001b[1;36m012\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m689\u001b[0m\u001b[1m}\u001b[0m - INFO - Training the model completed                \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:12:46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">013</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1529</span><span style=\"font-weight: bold\">}</span> - INFO - Loading the best model                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:12:46\u001b[0m,\u001b[1;36m013\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m1529\u001b[0m\u001b[1m}\u001b[0m - INFO - Loading the best model                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x33c9a3b50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabular_model.fit(train=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = tabular_model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ab3f05c2914df8a137f81acc061127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       4484307968.0        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       4484307968.0        </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_r2_score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7408820986747742     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      4484307968.0       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      4484307968.0       \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_r2_score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7408820986747742    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 66965\n",
      "R2 score: 0.7409\n"
     ]
    }
   ],
   "source": [
    "test_results = tabular_model.evaluate(df_test)\n",
    "print(f\"RMSE: {np.sqrt(test_results[0]['test_mean_squared_error']):.0f}\")\n",
    "print(f\"R2 score: {test_results[0]['test_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEJhRU18qX22"
   },
   "source": [
    "> Print out the corresponding rows in the dataframe for the top 25 test samples with the largest errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5ma5K9vKqZEq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>town</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "      <th>prediction</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92405</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>46 SENG POH ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.581977</td>\n",
       "      <td>2.309477</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>3 ROOM, Standard</td>\n",
       "      <td>50.166667</td>\n",
       "      <td>88.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>780000.0</td>\n",
       "      <td>3.993365e+05</td>\n",
       "      <td>380663.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90957</th>\n",
       "      <td>6</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT BATOK</td>\n",
       "      <td>288A BUKIT BATOK STREET 25</td>\n",
       "      <td>Bukit Batok</td>\n",
       "      <td>1.292540</td>\n",
       "      <td>10.763777</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>EXECUTIVE, Apartment</td>\n",
       "      <td>75.583333</td>\n",
       "      <td>144.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>968000.0</td>\n",
       "      <td>6.147756e+05</td>\n",
       "      <td>353224.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90601</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>BISHAN</td>\n",
       "      <td>251 BISHAN STREET 22</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.081018</td>\n",
       "      <td>6.939944</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>126.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>935000.0</td>\n",
       "      <td>6.199250e+05</td>\n",
       "      <td>315075.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90382</th>\n",
       "      <td>7</td>\n",
       "      <td>2021</td>\n",
       "      <td>BISHAN</td>\n",
       "      <td>251 BISHAN STREET 22</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.081018</td>\n",
       "      <td>6.939944</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>70.166667</td>\n",
       "      <td>121.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>945000.0</td>\n",
       "      <td>6.351278e+05</td>\n",
       "      <td>309872.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92533</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>2C BOON TIONG ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.656363</td>\n",
       "      <td>1.982722</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>78.083333</td>\n",
       "      <td>115.0</td>\n",
       "      <td>28 TO 30</td>\n",
       "      <td>1130000.0</td>\n",
       "      <td>8.260764e+05</td>\n",
       "      <td>303923.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90608</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>BISHAN</td>\n",
       "      <td>273B BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.776182</td>\n",
       "      <td>6.297489</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.833333</td>\n",
       "      <td>120.0</td>\n",
       "      <td>37 TO 39</td>\n",
       "      <td>1360000.0</td>\n",
       "      <td>1.061924e+06</td>\n",
       "      <td>298076.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105702</th>\n",
       "      <td>6</td>\n",
       "      <td>2021</td>\n",
       "      <td>QUEENSTOWN</td>\n",
       "      <td>150 MEI LING STREET</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>0.245207</td>\n",
       "      <td>4.709043</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.008342</td>\n",
       "      <td>EXECUTIVE, Apartment</td>\n",
       "      <td>73.416667</td>\n",
       "      <td>148.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>1235000.0</td>\n",
       "      <td>9.441447e+05</td>\n",
       "      <td>290855.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92340</th>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>56 HAVELOCK ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.451387</td>\n",
       "      <td>2.128424</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>90.750000</td>\n",
       "      <td>114.0</td>\n",
       "      <td>34 TO 36</td>\n",
       "      <td>1245000.0</td>\n",
       "      <td>9.546715e+05</td>\n",
       "      <td>290328.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89770</th>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>BEDOK</td>\n",
       "      <td>103 LENGKONG TIGA</td>\n",
       "      <td>Kembangan</td>\n",
       "      <td>0.622824</td>\n",
       "      <td>7.791966</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>66.333333</td>\n",
       "      <td>126.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>868000.0</td>\n",
       "      <td>5.781985e+05</td>\n",
       "      <td>289801.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90600</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>BISHAN</td>\n",
       "      <td>251 BISHAN STREET 22</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.081018</td>\n",
       "      <td>6.939944</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>69.666667</td>\n",
       "      <td>121.0</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>920000.0</td>\n",
       "      <td>6.303725e+05</td>\n",
       "      <td>289627.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91871</th>\n",
       "      <td>6</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>17 TIONG BAHRU ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.693391</td>\n",
       "      <td>2.058774</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>3 ROOM, Standard</td>\n",
       "      <td>50.583333</td>\n",
       "      <td>88.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>680888.0</td>\n",
       "      <td>3.918192e+05</td>\n",
       "      <td>289068.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92442</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>127D KIM TIAN ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.686789</td>\n",
       "      <td>2.664024</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>90.333333</td>\n",
       "      <td>113.0</td>\n",
       "      <td>16 TO 18</td>\n",
       "      <td>1165000.0</td>\n",
       "      <td>8.768001e+05</td>\n",
       "      <td>288199.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92504</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>49 KIM PONG ROAD</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.468378</td>\n",
       "      <td>2.365532</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>3 ROOM, Standard</td>\n",
       "      <td>50.166667</td>\n",
       "      <td>88.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>695000.0</td>\n",
       "      <td>4.080297e+05</td>\n",
       "      <td>286970.34375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88081</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>ANG MO KIO</td>\n",
       "      <td>310A ANG MO KIO AVENUE 1</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.860056</td>\n",
       "      <td>7.263401</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>90.166667</td>\n",
       "      <td>121.0</td>\n",
       "      <td>28 TO 30</td>\n",
       "      <td>1100000.0</td>\n",
       "      <td>8.133185e+05</td>\n",
       "      <td>286681.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92299</th>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>36 MOH GUAN TERRACE</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.490926</td>\n",
       "      <td>2.278805</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>3 ROOM, Standard</td>\n",
       "      <td>50.333333</td>\n",
       "      <td>88.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>690000.0</td>\n",
       "      <td>4.048715e+05</td>\n",
       "      <td>285128.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93825</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>CENTRAL AREA</td>\n",
       "      <td>4 TANJONG PAGAR PLAZA</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>0.451637</td>\n",
       "      <td>2.594828</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.103876</td>\n",
       "      <td>5 ROOM, Adjoined flat</td>\n",
       "      <td>54.583333</td>\n",
       "      <td>118.0</td>\n",
       "      <td>16 TO 18</td>\n",
       "      <td>938000.0</td>\n",
       "      <td>6.550211e+05</td>\n",
       "      <td>282978.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100836</th>\n",
       "      <td>6</td>\n",
       "      <td>2021</td>\n",
       "      <td>KALLANG/WHAMPOA</td>\n",
       "      <td>39 JALAN BAHAGIA</td>\n",
       "      <td>Boon Keng</td>\n",
       "      <td>0.998313</td>\n",
       "      <td>3.304953</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.053004</td>\n",
       "      <td>3 ROOM, Terrace</td>\n",
       "      <td>50.083333</td>\n",
       "      <td>210.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>1268000.0</td>\n",
       "      <td>9.853905e+05</td>\n",
       "      <td>282609.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112128</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>TAMPINES</td>\n",
       "      <td>156 TAMPINES STREET 12</td>\n",
       "      <td>Tampines</td>\n",
       "      <td>0.370873</td>\n",
       "      <td>12.479752</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>EXECUTIVE, Maisonette</td>\n",
       "      <td>61.750000</td>\n",
       "      <td>148.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>998000.0</td>\n",
       "      <td>7.192398e+05</td>\n",
       "      <td>278760.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91694</th>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>35 LIM LIAK STREET</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.481551</td>\n",
       "      <td>2.262574</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>3 ROOM, Standard</td>\n",
       "      <td>50.833333</td>\n",
       "      <td>88.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>680000.0</td>\n",
       "      <td>4.015675e+05</td>\n",
       "      <td>278432.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90432</th>\n",
       "      <td>8</td>\n",
       "      <td>2021</td>\n",
       "      <td>BISHAN</td>\n",
       "      <td>275A BISHAN STREET 24</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.827889</td>\n",
       "      <td>6.370404</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.916667</td>\n",
       "      <td>120.0</td>\n",
       "      <td>25 TO 27</td>\n",
       "      <td>1280000.0</td>\n",
       "      <td>1.003262e+06</td>\n",
       "      <td>276738.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93895</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>CENTRAL AREA</td>\n",
       "      <td>3 TANJONG PAGAR PLAZA</td>\n",
       "      <td>Tanjong Pagar</td>\n",
       "      <td>0.490378</td>\n",
       "      <td>2.630876</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.103876</td>\n",
       "      <td>5 ROOM, Adjoined flat</td>\n",
       "      <td>54.250000</td>\n",
       "      <td>139.0</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>958000.0</td>\n",
       "      <td>6.813354e+05</td>\n",
       "      <td>276664.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90521</th>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>BISHAN</td>\n",
       "      <td>237 BISHAN STREET 22</td>\n",
       "      <td>Bishan</td>\n",
       "      <td>0.947205</td>\n",
       "      <td>6.663943</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>5 ROOM, Improved</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>121.0</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>988000.0</td>\n",
       "      <td>7.143948e+05</td>\n",
       "      <td>273605.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101237</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>KALLANG/WHAMPOA</td>\n",
       "      <td>8 BOON KENG ROAD</td>\n",
       "      <td>Bendemeer</td>\n",
       "      <td>0.352251</td>\n",
       "      <td>2.587444</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>5 ROOM, DBSS</td>\n",
       "      <td>88.250000</td>\n",
       "      <td>119.0</td>\n",
       "      <td>40 TO 42</td>\n",
       "      <td>1268000.0</td>\n",
       "      <td>9.956690e+05</td>\n",
       "      <td>272331.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93670</th>\n",
       "      <td>12</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT TIMAH</td>\n",
       "      <td>6 TOH YI DRIVE</td>\n",
       "      <td>Beauty World</td>\n",
       "      <td>0.428356</td>\n",
       "      <td>8.948410</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>EXECUTIVE, Maisonette</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>154.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>1238000.0</td>\n",
       "      <td>9.660116e+05</td>\n",
       "      <td>271988.37500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92379</th>\n",
       "      <td>11</td>\n",
       "      <td>2021</td>\n",
       "      <td>BUKIT MERAH</td>\n",
       "      <td>48 MOH GUAN TERRACE</td>\n",
       "      <td>Tiong Bahru</td>\n",
       "      <td>0.538563</td>\n",
       "      <td>2.345844</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.047782</td>\n",
       "      <td>3 ROOM, Standard</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>640000.0</td>\n",
       "      <td>3.702984e+05</td>\n",
       "      <td>269701.59375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year             town                full_address  \\\n",
       "92405      11  2021      BUKIT MERAH            46 SENG POH ROAD   \n",
       "90957       6  2021      BUKIT BATOK  288A BUKIT BATOK STREET 25   \n",
       "90601      12  2021           BISHAN        251 BISHAN STREET 22   \n",
       "90382       7  2021           BISHAN        251 BISHAN STREET 22   \n",
       "92533      12  2021      BUKIT MERAH          2C BOON TIONG ROAD   \n",
       "90608      12  2021           BISHAN       273B BISHAN STREET 24   \n",
       "105702      6  2021       QUEENSTOWN         150 MEI LING STREET   \n",
       "92340      10  2021      BUKIT MERAH            56 HAVELOCK ROAD   \n",
       "89770      10  2021            BEDOK           103 LENGKONG TIGA   \n",
       "90600      12  2021           BISHAN        251 BISHAN STREET 22   \n",
       "91871       6  2021      BUKIT MERAH         17 TIONG BAHRU ROAD   \n",
       "92442      11  2021      BUKIT MERAH          127D KIM TIAN ROAD   \n",
       "92504      12  2021      BUKIT MERAH            49 KIM PONG ROAD   \n",
       "88081       8  2021       ANG MO KIO    310A ANG MO KIO AVENUE 1   \n",
       "92299      10  2021      BUKIT MERAH         36 MOH GUAN TERRACE   \n",
       "93825       8  2021     CENTRAL AREA       4 TANJONG PAGAR PLAZA   \n",
       "100836      6  2021  KALLANG/WHAMPOA            39 JALAN BAHAGIA   \n",
       "112128     12  2021         TAMPINES      156 TAMPINES STREET 12   \n",
       "91694       4  2021      BUKIT MERAH          35 LIM LIAK STREET   \n",
       "90432       8  2021           BISHAN       275A BISHAN STREET 24   \n",
       "93895      11  2021     CENTRAL AREA       3 TANJONG PAGAR PLAZA   \n",
       "90521      10  2021           BISHAN        237 BISHAN STREET 22   \n",
       "101237     11  2021  KALLANG/WHAMPOA            8 BOON KENG ROAD   \n",
       "93670      12  2021      BUKIT TIMAH              6 TOH YI DRIVE   \n",
       "92379      11  2021      BUKIT MERAH         48 MOH GUAN TERRACE   \n",
       "\n",
       "          nearest_stn  dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "92405     Tiong Bahru             0.581977       2.309477           0.016807   \n",
       "90957     Bukit Batok             1.292540      10.763777           0.016807   \n",
       "90601      Ang Mo Kio             1.081018       6.939944           0.016807   \n",
       "90382      Ang Mo Kio             1.081018       6.939944           0.016807   \n",
       "92533     Tiong Bahru             0.656363       1.982722           0.016807   \n",
       "90608          Bishan             0.776182       6.297489           0.033613   \n",
       "105702     Queenstown             0.245207       4.709043           0.016807   \n",
       "92340     Tiong Bahru             0.451387       2.128424           0.016807   \n",
       "89770       Kembangan             0.622824       7.791966           0.016807   \n",
       "90600      Ang Mo Kio             1.081018       6.939944           0.016807   \n",
       "91871     Tiong Bahru             0.693391       2.058774           0.016807   \n",
       "92442     Tiong Bahru             0.686789       2.664024           0.016807   \n",
       "92504     Tiong Bahru             0.468378       2.365532           0.016807   \n",
       "88081      Ang Mo Kio             0.860056       7.263401           0.016807   \n",
       "92299     Tiong Bahru             0.490926       2.278805           0.016807   \n",
       "93825   Tanjong Pagar             0.451637       2.594828           0.016807   \n",
       "100836      Boon Keng             0.998313       3.304953           0.016807   \n",
       "112128       Tampines             0.370873      12.479752           0.033613   \n",
       "91694     Tiong Bahru             0.481551       2.262574           0.016807   \n",
       "90432          Bishan             0.827889       6.370404           0.033613   \n",
       "93895   Tanjong Pagar             0.490378       2.630876           0.016807   \n",
       "90521          Bishan             0.947205       6.663943           0.033613   \n",
       "101237      Bendemeer             0.352251       2.587444           0.016807   \n",
       "93670    Beauty World             0.428356       8.948410           0.016807   \n",
       "92379     Tiong Bahru             0.538563       2.345844           0.016807   \n",
       "\n",
       "        eigenvector_centrality        flat_model_type  remaining_lease_years  \\\n",
       "92405                 0.047782       3 ROOM, Standard              50.166667   \n",
       "90957                 0.000217   EXECUTIVE, Apartment              75.583333   \n",
       "90601                 0.006243       5 ROOM, Improved              69.750000   \n",
       "90382                 0.006243       5 ROOM, Improved              70.166667   \n",
       "92533                 0.047782       5 ROOM, Improved              78.083333   \n",
       "90608                 0.015854           5 ROOM, DBSS              88.833333   \n",
       "105702                0.008342   EXECUTIVE, Apartment              73.416667   \n",
       "92340                 0.047782       5 ROOM, Improved              90.750000   \n",
       "89770                 0.002799       5 ROOM, Improved              66.333333   \n",
       "90600                 0.006243       5 ROOM, Improved              69.666667   \n",
       "91871                 0.047782       3 ROOM, Standard              50.583333   \n",
       "92442                 0.047782       5 ROOM, Improved              90.333333   \n",
       "92504                 0.047782       3 ROOM, Standard              50.166667   \n",
       "88081                 0.006243       5 ROOM, Improved              90.166667   \n",
       "92299                 0.047782       3 ROOM, Standard              50.333333   \n",
       "93825                 0.103876  5 ROOM, Adjoined flat              54.583333   \n",
       "100836                0.053004        3 ROOM, Terrace              50.083333   \n",
       "112128                0.000229  EXECUTIVE, Maisonette              61.750000   \n",
       "91694                 0.047782       3 ROOM, Standard              50.833333   \n",
       "90432                 0.015854           5 ROOM, DBSS              88.916667   \n",
       "93895                 0.103876  5 ROOM, Adjoined flat              54.250000   \n",
       "90521                 0.015854       5 ROOM, Improved              69.583333   \n",
       "101237                0.004414           5 ROOM, DBSS              88.250000   \n",
       "93670                 0.001358  EXECUTIVE, Maisonette              66.666667   \n",
       "92379                 0.047782       3 ROOM, Standard              50.250000   \n",
       "\n",
       "        floor_area_sqm storey_range  resale_price    prediction         error  \n",
       "92405             88.0     01 TO 03      780000.0  3.993365e+05  380663.50000  \n",
       "90957            144.0     10 TO 12      968000.0  6.147756e+05  353224.43750  \n",
       "90601            126.0     04 TO 06      935000.0  6.199250e+05  315075.00000  \n",
       "90382            121.0     10 TO 12      945000.0  6.351278e+05  309872.25000  \n",
       "92533            115.0     28 TO 30     1130000.0  8.260764e+05  303923.62500  \n",
       "90608            120.0     37 TO 39     1360000.0  1.061924e+06  298076.25000  \n",
       "105702           148.0     10 TO 12     1235000.0  9.441447e+05  290855.31250  \n",
       "92340            114.0     34 TO 36     1245000.0  9.546715e+05  290328.50000  \n",
       "89770            126.0     10 TO 12      868000.0  5.781985e+05  289801.50000  \n",
       "90600            121.0     07 TO 09      920000.0  6.303725e+05  289627.50000  \n",
       "91871             88.0     01 TO 03      680888.0  3.918192e+05  289068.75000  \n",
       "92442            113.0     16 TO 18     1165000.0  8.768001e+05  288199.87500  \n",
       "92504             88.0     01 TO 03      695000.0  4.080297e+05  286970.34375  \n",
       "88081            121.0     28 TO 30     1100000.0  8.133185e+05  286681.50000  \n",
       "92299             88.0     01 TO 03      690000.0  4.048715e+05  285128.50000  \n",
       "93825            118.0     16 TO 18      938000.0  6.550211e+05  282978.87500  \n",
       "100836           210.0     01 TO 03     1268000.0  9.853905e+05  282609.50000  \n",
       "112128           148.0     01 TO 03      998000.0  7.192398e+05  278760.25000  \n",
       "91694             88.0     01 TO 03      680000.0  4.015675e+05  278432.53125  \n",
       "90432            120.0     25 TO 27     1280000.0  1.003262e+06  276738.00000  \n",
       "93895            139.0     07 TO 09      958000.0  6.813354e+05  276664.62500  \n",
       "90521            121.0     07 TO 09      988000.0  7.143948e+05  273605.25000  \n",
       "101237           119.0     40 TO 42     1268000.0  9.956690e+05  272331.00000  \n",
       "93670            154.0     04 TO 06     1238000.0  9.660116e+05  271988.37500  \n",
       "92379             77.0     04 TO 06      640000.0  3.702984e+05  269701.59375  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = df_test.copy()\n",
    "results_df['prediction'] = pred_df['resale_price_prediction']\n",
    "results_df['error'] = abs(results_df['resale_price'] - results_df['prediction'])\n",
    "\n",
    "top_errors = results_df.sort_values(by='error', ascending=False).head(25)\n",
    "display(top_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model.save_model(\"models/b1_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q2 (10 marks)\n",
    "---\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-widedeep==1.6.5 in ./.venv/lib/python3.11/site-packages (1.6.5)\n",
      "Requirement already satisfied: pandas>=1.3.5 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (2.2.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.6 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (1.26.4)\n",
      "Requirement already satisfied: scipy<=1.12.0,>=1.7.3 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (1.6.1)\n",
      "Requirement already satisfied: gensim in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (4.3.3)\n",
      "Requirement already satisfied: spacy in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (3.8.4)\n",
      "Requirement already satisfied: opencv-contrib-python>=4.9.0.80 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (4.11.0.86)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (0.21.0)\n",
      "Requirement already satisfied: einops in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (0.7.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (1.17.2)\n",
      "Requirement already satisfied: torchmetrics>=1.3.1 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (1.5.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (19.0.1)\n",
      "Requirement already satisfied: fastparquet>=2024.2.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (2024.11.0)\n",
      "Requirement already satisfied: transformers>=4.37.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (4.49.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.3.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (3.4.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./.venv/lib/python3.11/site-packages (from pytorch-widedeep==1.6.5) (0.2.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in ./.venv/lib/python3.11/site-packages (from fastparquet>=2024.2.0->pytorch-widedeep==1.6.5) (2.9.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from fastparquet>=2024.2.0->pytorch-widedeep==1.6.5) (2025.2.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from fastparquet>=2024.2.0->pytorch-widedeep==1.6.5) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas>=1.3.5->pytorch-widedeep==1.6.5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.3.5->pytorch-widedeep==1.6.5) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.3.5->pytorch-widedeep==1.6.5) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep==1.6.5) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn>=1.0.2->pytorch-widedeep==1.6.5) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=2.3.0->pytorch-widedeep==1.6.5) (0.29.2)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (from sentence-transformers>=2.3.0->pytorch-widedeep==1.6.5) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->pytorch-widedeep==1.6.5) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->pytorch-widedeep==1.6.5) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->pytorch-widedeep==1.6.5) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->pytorch-widedeep==1.6.5) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=2.0.0->pytorch-widedeep==1.6.5) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->pytorch-widedeep==1.6.5) (1.3.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in ./.venv/lib/python3.11/site-packages (from torchmetrics>=1.3.1->pytorch-widedeep==1.6.5) (0.14.0)\n",
      "Collecting torch>=2.0.0 (from pytorch-widedeep==1.6.5)\n",
      "  Using cached torch-2.6.0-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.0->pytorch-widedeep==1.6.5) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.0->pytorch-widedeep==1.6.5) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.0->pytorch-widedeep==1.6.5) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.0->pytorch-widedeep==1.6.5) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers>=4.37.0->pytorch-widedeep==1.6.5) (0.5.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.venv/lib/python3.11/site-packages (from gensim->pytorch-widedeep==1.6.5) (7.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (0.15.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (2.10.6)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (75.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.venv/lib/python3.11/site-packages (from spacy->pytorch-widedeep==1.6.5) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in ./.venv/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep==1.6.5) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep==1.6.5) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep==1.6.5) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep==1.6.5) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep==1.6.5) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep==1.6.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep==1.6.5) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers>=4.37.0->pytorch-widedeep==1.6.5) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in ./.venv/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy->pytorch-widedeep==1.6.5) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.venv/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy->pytorch-widedeep==1.6.5) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep==1.6.5) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep==1.6.5) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep==1.6.5) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in ./.venv/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy->pytorch-widedeep==1.6.5) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep==1.6.5) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in ./.venv/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep==1.6.5) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep==1.6.5) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep==1.6.5) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep==1.6.5) (0.1.2)\n",
      "Using cached torch-2.6.0-cp311-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "Successfully installed torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"pytorch-widedeep==1.6.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data（validation set is not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df_train = df[df['year'] <= 2020]\n",
    "df_test = df[df['year'] >= 2021]\n",
    "\n",
    "X_train = df_train.drop(\"resale_price\", axis=1)\n",
    "y_train = df_train[\"resale_price\"]\n",
    "X_test = df_test.drop(\"resale_price\", axis=1)\n",
    "y_test = df_test[\"resale_price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 hidden layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 60 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n"
     ]
    }
   ],
   "source": [
    "continuous_cols=[\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\",\n",
    "                     \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n",
    "categorical_cols=[\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=categorical_cols,\n",
    "    continuous_cols=continuous_cols\n",
    ")\n",
    "\n",
    "X_tab = tab_preprocessor.fit_transform(X_train)\n",
    "X_tab_test = tab_preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[200, 100],\n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|██████████| 1366/1366 [00:15<00:00, 90.78it/s, loss=1.84e+5, metrics={'r2': -1.2531}] \n",
      "epoch 2: 100%|██████████| 1366/1366 [00:11<00:00, 115.87it/s, loss=9.93e+4, metrics={'r2': 0.4928}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:11<00:00, 116.34it/s, loss=8.01e+4, metrics={'r2': 0.6769}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:11<00:00, 116.38it/s, loss=6.71e+4, metrics={'r2': 0.789}] \n",
      "epoch 5: 100%|██████████| 1366/1366 [00:11<00:00, 116.26it/s, loss=6.21e+4, metrics={'r2': 0.8243}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:12<00:00, 112.93it/s, loss=6e+4, metrics={'r2': 0.838}]    \n",
      "epoch 7: 100%|██████████| 1366/1366 [00:11<00:00, 118.30it/s, loss=5.88e+4, metrics={'r2': 0.8447}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:11<00:00, 116.75it/s, loss=5.77e+4, metrics={'r2': 0.8506}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:12<00:00, 111.96it/s, loss=5.67e+4, metrics={'r2': 0.856}] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:12<00:00, 112.16it/s, loss=5.6e+4, metrics={'r2': 0.8594}] \n",
      "epoch 11: 100%|██████████| 1366/1366 [00:12<00:00, 111.98it/s, loss=5.53e+4, metrics={'r2': 0.8632}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:11<00:00, 115.05it/s, loss=5.49e+4, metrics={'r2': 0.8651}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:11<00:00, 117.23it/s, loss=5.43e+4, metrics={'r2': 0.8679}]\n",
      "epoch 14: 100%|██████████| 1366/1366 [00:11<00:00, 119.27it/s, loss=5.39e+4, metrics={'r2': 0.8699}]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:11<00:00, 119.88it/s, loss=5.34e+4, metrics={'r2': 0.8726}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:11<00:00, 116.23it/s, loss=5.3e+4, metrics={'r2': 0.8744}] \n",
      "epoch 17: 100%|██████████| 1366/1366 [00:12<00:00, 111.92it/s, loss=5.26e+4, metrics={'r2': 0.8766}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:12<00:00, 111.81it/s, loss=5.21e+4, metrics={'r2': 0.8788}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:12<00:00, 111.54it/s, loss=5.16e+4, metrics={'r2': 0.8812}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:12<00:00, 112.62it/s, loss=5.12e+4, metrics={'r2': 0.883}] \n",
      "epoch 21: 100%|██████████| 1366/1366 [00:12<00:00, 108.67it/s, loss=5.08e+4, metrics={'r2': 0.885}] \n",
      "epoch 22: 100%|██████████| 1366/1366 [00:11<00:00, 118.12it/s, loss=5.03e+4, metrics={'r2': 0.8876}]\n",
      "epoch 23: 100%|██████████| 1366/1366 [00:12<00:00, 110.86it/s, loss=5.01e+4, metrics={'r2': 0.8882}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:12<00:00, 107.68it/s, loss=4.95e+4, metrics={'r2': 0.891}] \n",
      "epoch 25: 100%|██████████| 1366/1366 [00:12<00:00, 109.30it/s, loss=4.94e+4, metrics={'r2': 0.8912}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:12<00:00, 109.42it/s, loss=4.92e+4, metrics={'r2': 0.8922}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:12<00:00, 109.11it/s, loss=4.9e+4, metrics={'r2': 0.893}]  \n",
      "epoch 28: 100%|██████████| 1366/1366 [00:12<00:00, 109.53it/s, loss=4.9e+4, metrics={'r2': 0.8933}] \n",
      "epoch 29: 100%|██████████| 1366/1366 [00:12<00:00, 108.83it/s, loss=4.88e+4, metrics={'r2': 0.894}] \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:12<00:00, 109.47it/s, loss=4.87e+4, metrics={'r2': 0.8944}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:12<00:00, 108.72it/s, loss=4.84e+4, metrics={'r2': 0.8957}]\n",
      "epoch 32: 100%|██████████| 1366/1366 [00:12<00:00, 108.75it/s, loss=4.83e+4, metrics={'r2': 0.8961}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:12<00:00, 108.42it/s, loss=4.83e+4, metrics={'r2': 0.8961}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:12<00:00, 107.55it/s, loss=4.81e+4, metrics={'r2': 0.8969}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:12<00:00, 108.17it/s, loss=4.81e+4, metrics={'r2': 0.8965}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:12<00:00, 108.35it/s, loss=4.82e+4, metrics={'r2': 0.8964}]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:12<00:00, 108.27it/s, loss=4.8e+4, metrics={'r2': 0.8971}] \n",
      "epoch 38: 100%|██████████| 1366/1366 [00:12<00:00, 108.31it/s, loss=4.79e+4, metrics={'r2': 0.8977}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:12<00:00, 109.07it/s, loss=4.77e+4, metrics={'r2': 0.8986}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:12<00:00, 108.56it/s, loss=4.78e+4, metrics={'r2': 0.8981}]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:12<00:00, 108.79it/s, loss=4.77e+4, metrics={'r2': 0.8984}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:12<00:00, 108.81it/s, loss=4.76e+4, metrics={'r2': 0.8988}]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:12<00:00, 108.31it/s, loss=4.75e+4, metrics={'r2': 0.8995}]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:12<00:00, 108.65it/s, loss=4.74e+4, metrics={'r2': 0.8995}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:12<00:00, 108.61it/s, loss=4.76e+4, metrics={'r2': 0.8989}]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:12<00:00, 108.91it/s, loss=4.76e+4, metrics={'r2': 0.8986}]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:12<00:00, 108.41it/s, loss=4.74e+4, metrics={'r2': 0.8996}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:12<00:00, 108.66it/s, loss=4.74e+4, metrics={'r2': 0.8996}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:12<00:00, 108.65it/s, loss=4.73e+4, metrics={'r2': 0.9002}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:12<00:00, 108.50it/s, loss=4.72e+4, metrics={'r2': 0.9005}]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:12<00:00, 108.40it/s, loss=4.72e+4, metrics={'r2': 0.9007}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:12<00:00, 108.73it/s, loss=4.71e+4, metrics={'r2': 0.9008}]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:12<00:00, 108.92it/s, loss=4.7e+4, metrics={'r2': 0.9012}] \n",
      "epoch 54: 100%|██████████| 1366/1366 [00:12<00:00, 108.56it/s, loss=4.7e+4, metrics={'r2': 0.9013}] \n",
      "epoch 55: 100%|██████████| 1366/1366 [00:12<00:00, 108.26it/s, loss=4.72e+4, metrics={'r2': 0.9003}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:12<00:00, 108.55it/s, loss=4.71e+4, metrics={'r2': 0.9011}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:12<00:00, 108.48it/s, loss=4.67e+4, metrics={'r2': 0.9024}]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:12<00:00, 108.21it/s, loss=4.67e+4, metrics={'r2': 0.9027}]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:12<00:00, 107.06it/s, loss=4.69e+4, metrics={'r2': 0.9015}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:12<00:00, 108.29it/s, loss=4.67e+4, metrics={'r2': 0.9029}]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, objective='rmse', metrics=[R2Score], num_workers=0)\n",
    "trainer.fit(X_tab=X_tab, target=y_train, n_epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(path='./models', model_filename='b2_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:02<00:00, 402.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  106457.01843640397\n",
      "R2:  0.6040438184895371\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(X_tab=X_tab_test,batch_size=64)\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "print(\"R2: \", r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q3 (10 marks)\n",
    "---\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in ./.venv/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (from captum) (3.10.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.6 in ./.venv/lib/python3.11/site-packages (from captum) (2.6.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from captum) (4.67.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.6->captum) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.6->captum) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.6->captum) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.6->captum) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch>=1.6->captum) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.6->captum) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"captum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, use the train set (year 2020 and before) and test set (year 2021) following the splits in Question B1 (validation set is not required here). To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Standardise the features via **StandardScaler** (fit to training set, then transform all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "continuous_cols=[\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\",\n",
    "                     \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n",
    "categorical_cols=[\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
    "\n",
    "df.drop(categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "df_train = df[df['year'] <= 2020]\n",
    "df_test = df[df['year'] == 2021] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Input x Gradients, Integrated Gradients, DeepLift, GradientSHAP, Feature Ablation. To avoid long running time, you can limit the analysis to the first 1000 samples in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "target = ['resale_price']\n",
    "n_epochs = 60\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "hidden_neuron = 5\n",
    "\n",
    "X_train = torch.tensor(df_train[continuous_cols].values).float()\n",
    "y_train = torch.tensor(df_train[target].values).view(-1, 1).float()\n",
    "\n",
    "X_test = torch.tensor(df_test[continuous_cols].values).float()\n",
    "y_test = torch.tensor(df_test[target].values).view(-1, 1).float()\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b3_model(\n",
       "  (lin1): Linear(in_features=6, out_features=5, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (lin2): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (lin3): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (relu3): ReLU()\n",
       "  (lin4): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class b3_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(6, hidden_neuron)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_neuron, hidden_neuron)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.lin3 = nn.Linear(hidden_neuron, hidden_neuron)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.lin4 = nn.Linear(hidden_neuron, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.lin4(self.relu3(self.lin3(self.relu2(self.lin2(self.relu1(self.lin1(input)))))))\n",
    "    \n",
    "model = b3_model()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train(model_inp, optimizer, num_epochs = n_epochs):\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # forward pass\n",
    "            outputs = model_inp(inputs)\n",
    "            # defining loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # computing gradients\n",
    "            loss.backward()\n",
    "            # accumulating running loss\n",
    "            running_loss += loss.item()\n",
    "            # updated weights based on computed gradients\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:    \n",
    "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                  (epoch + 1, num_epochs, running_loss))\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model from: models/b3_model\n"
     ]
    }
   ],
   "source": [
    "def train_load_save_model(model_obj, model_path):\n",
    "    if path.isfile(model_path):\n",
    "        # load model\n",
    "        print('Loading pre-trained model from: {}'.format(model_path))\n",
    "        model_obj.load_state_dict(torch.load(model_path))\n",
    "    else:    \n",
    "        # train model\n",
    "        train(model_obj)\n",
    "        print('Finished training the model. Saving the model to the path: {}'.format(model_path))\n",
    "        torch.save(model_obj.state_dict(), model_path)\n",
    "\n",
    "SAVED_MODEL_PATH = 'models/b3_model'\n",
    "train_load_save_model(model, SAVED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  90977.40413970932\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "\n",
    "print('RMSE: ', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sal = Saliency(model)\n",
    "ixg = InputXGradient(model)\n",
    "ig = IntegratedGradients(model)\n",
    "gs = GradientShap(model)\n",
    "fa = FeatureAblation(model)\n",
    "\n",
    "shortened_X_test = X_test[:n]\n",
    "\n",
    "sal_attr_test = sal.attribute(shortened_X_test)\n",
    "ixg_attr_test = ixg.attribute(shortened_X_test)\n",
    "ig_attr_test = ig.attribute(shortened_X_test, n_steps=50)\n",
    "gs_attr_test = gs.attribute(shortened_X_test, X_train)\n",
    "fa_attr_test = fa.attribute(shortened_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_names = df_train.drop(target, axis=1).columns\n",
    "\n",
    "x_axis_data = np.arange(X_test.shape[1])\n",
    "x_axis_data_labels = list(map(lambda idx: feature_names[idx], x_axis_data))\n",
    "\n",
    "sal_attr_test_sum = sal_attr_test.detach().numpy().sum(0)\n",
    "sal_attr_test_norm_sum = sal_attr_test_sum / np.linalg.norm(sal_attr_test_sum, ord=1)\n",
    "\n",
    "ixg_attr_test_sum = ixg_attr_test.detach().numpy().sum(0)\n",
    "ixg_attr_test_norm_sum = ixg_attr_test_sum / np.linalg.norm(ixg_attr_test_sum, ord=1)\n",
    "\n",
    "ig_attr_test_sum = ig_attr_test.detach().numpy().sum(0)\n",
    "ig_attr_test_norm_sum = ig_attr_test_sum / np.linalg.norm(ig_attr_test_sum, ord=1)\n",
    "\n",
    "gs_attr_test_sum = gs_attr_test.detach().numpy().sum(0)\n",
    "gs_attr_test_norm_sum = gs_attr_test_sum / np.linalg.norm(gs_attr_test_sum, ord=1)\n",
    "\n",
    "fa_attr_test_sum = fa_attr_test.detach().numpy().sum(0)\n",
    "fa_attr_test_norm_sum = fa_attr_test_sum / np.linalg.norm(fa_attr_test_sum, ord=1)\n",
    "\n",
    "width = 0.14\n",
    "legends = ['Saliency','Input x Gradients', 'Integrated Gradients', 'GradientSHAP', 'Feature Ablation']\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_title('Comparing input feature importances across multiple algorithms and learned weights')\n",
    "ax.set_ylabel('Attributions')\n",
    "\n",
    "FONT_SIZE = 16\n",
    "plt.rc('font', size=FONT_SIZE)            # fontsize of the text sizes\n",
    "plt.rc('axes', titlesize=FONT_SIZE)       # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=FONT_SIZE)       # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "ax.bar(x_axis_data, sal_attr_test_norm_sum, width, align='center', alpha=0.8, color='#f5a142')\n",
    "ax.bar(x_axis_data + width, ixg_attr_test_norm_sum, width, align='center', alpha=0.8, color='#f5428d')\n",
    "ax.bar(x_axis_data + 2 * width, ig_attr_test_norm_sum, width, align='center', alpha=0.8, color='#eb5e7c')\n",
    "ax.bar(x_axis_data + 3 * width, gs_attr_test_norm_sum, width, align='center',  alpha=0.8, color='#4260f5')\n",
    "ax.bar(x_axis_data + 4 * width, fa_attr_test_norm_sum, width, align='center', alpha=1.0, color='#49ba81')\n",
    "\n",
    "ax.autoscale_view()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_xticks(x_axis_data + 0.5)\n",
    "ax.set_xticklabels(x_axis_data_labels)\n",
    "\n",
    "plt.legend(legends, loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum to build up your understanding of the difference of various explainability algorithms. Based on your plot, identify the three most important features for regression. Explain how each of these features influences the regression outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q4 (10 marks)\n",
    "---\n",
    "\n",
    "Model degradation is a common issue faced when deploying machine learning models (including neural networks) in the real world. New data points could exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. For instance, housing prices in Singapore have been increasing and the Singapore government has introduced 3 rounds of cooling measures over the past years (16 December 2021, 30 September 2022, 27 April 2023).\n",
    "\n",
    "In such situations, the distribution of the new data points could differ from the original data distribution which the models were trained on. Recall that machine learning models often work with the assumption that the test distribution should be similar to train distribution. When this assumption is violated, model performance will be adversely impacted.  In the last part of this assignment, we will investigate to what extent model degradation has occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: alibi-detect in ./.venv/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (3.10.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.0.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (2.2.3)\n",
      "Requirement already satisfied: Pillow<11.0.0,>=5.4.1 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (10.4.0)\n",
      "Requirement already satisfied: opencv-python<5.0.0,>=3.2.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (4.11.0.86)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.3.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (1.12.0)\n",
      "Requirement already satisfied: scikit-image<0.23,>=0.19 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (0.22.0)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=0.20.2 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (1.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.0.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (4.49.0)\n",
      "Requirement already satisfied: dill<0.4.0,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (0.3.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (2.32.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.8.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (2.10.6)\n",
      "Requirement already satisfied: toml<1.0.0,>=0.10.1 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (0.10.2)\n",
      "Requirement already satisfied: catalogue<3.0.0,>=2.0.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (2.0.10)\n",
      "Requirement already satisfied: numba!=0.54.0,<0.60.0,>=0.50.0 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (0.59.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from alibi-detect) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi-detect) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in ./.venv/lib/python3.11/site-packages (from numba!=0.54.0,<0.60.0,>=0.50.0->alibi-detect) (0.42.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=1.0.0->alibi-detect) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas<3.0.0,>=1.0.0->alibi-detect) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=1.8.0->alibi-detect) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=1.8.0->alibi-detect) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.21.0->alibi-detect) (2025.1.31)\n",
      "Requirement already satisfied: networkx>=2.8 in ./.venv/lib/python3.11/site-packages (from scikit-image<0.23,>=0.19->alibi-detect) (3.4.2)\n",
      "Requirement already satisfied: imageio>=2.27 in ./.venv/lib/python3.11/site-packages (from scikit-image<0.23,>=0.19->alibi-detect) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.venv/lib/python3.11/site-packages (from scikit-image<0.23,>=0.19->alibi-detect) (2025.2.18)\n",
      "Requirement already satisfied: lazy_loader>=0.3 in ./.venv/lib/python3.11/site-packages (from scikit-image<0.23,>=0.19->alibi-detect) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=0.20.2->alibi-detect) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn<2.0.0,>=0.20.2->alibi-detect) (3.5.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (0.29.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.0.0->alibi-detect) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers<5.0.0,>=4.0.0->alibi-detect) (2025.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.0.0->alibi-detect) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"alibi-detect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import TabularDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2022 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:15:55</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">838</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">171</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:15:55\u001b[0m,\u001b[1;36m838\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m171\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:15:55</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">840</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">342</span><span style=\"font-weight: bold\">}</span> - INFO - Preparing the Trainer                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m03\u001b[0m-\u001b[1;36m11\u001b[0m \u001b[1;92m10:15:55\u001b[0m,\u001b[1;36m840\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m342\u001b[0m\u001b[1m}\u001b[0m - INFO - Preparing the Trainer                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96348f538b84cee968b0d004c82243b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       13752151040.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       13752151040.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_r2_score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5079993009567261     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      13752151040.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      13752151040.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_r2_score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5079993009567261    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 117270\n",
      "R2 score: 0.5080\n"
     ]
    }
   ],
   "source": [
    "b1_model = TabularModel.load_model(\"models/b1_model\")\n",
    "\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df_test = df[df[\"year\"] == 2022]\n",
    "\n",
    "pred = b1_model.predict(df_test)\n",
    "test_results = b1_model.evaluate(df_test)\n",
    "print(f\"RMSE: {np.sqrt(test_results[0]['test_mean_squared_error']):.0f}\")\n",
    "print(f\"R2 score: {test_results[0]['test_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2023 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb015eb12194cf3af30598749ddb5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuki/Library/Mobile Documents/com~apple~CloudDocs/NTU/Y2S2/SC4001/Projects/Assignment1/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       22023643136.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test_mean_squared_error  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       22023643136.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_r2_score       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.22778698801994324    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      22023643136.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test_mean_squared_error \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      22023643136.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_r2_score      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.22778698801994324   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 148404\n",
      "R2 score: 0.2278\n"
     ]
    }
   ],
   "source": [
    "df_test = df[df[\"year\"] == 2023]\n",
    "\n",
    "pred = b1_model.predict(df_test)\n",
    "test_results = tabular_model.evaluate(df_test)\n",
    "print(f\"RMSE: {np.sqrt(test_results[0]['test_mean_squared_error']):.0f}\")\n",
    "print(f\"R2 score: {test_results[0]['test_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Did model degradation occur for the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model degradation could be caused by [various data distribution shifts](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html#data-shift-types): covariate shift (features), label shift and/or concept drift (altered relationship between features and labels).\n",
    "There are various conflicting terminologies in the [literature](https://www.sciencedirect.com/science/article/pii/S0950705122002854#tbl1). Let’s stick to this reference for this assignment.\n",
    "\n",
    "> Using the **Alibi Detect** library, apply the **TabularDrift** function with the training data (year 2020 and before) used as the reference and **detect which features have drifted** in the 2023 test dataset. Before running the statistical tests, ensure you **sample 1000 data points** each from the train and test data. Do not use the whole train/test data. (Hint: use this example as a guide https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols=[\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\",\n",
    "                     \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n",
    "categorical_cols=[\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
    "target = ['resale_price']\n",
    "\n",
    "category_map = {}\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    category_map[i] = df[col].unique().tolist()\n",
    "\n",
    "n_ref = 1000\n",
    "n_test = 1000\n",
    "\n",
    "df_ref = df[df[\"year\"] <= 2020]\n",
    "df_ref = df_ref[:n_ref]\n",
    "\n",
    "df_test = df[df[\"year\"] == 2023]\n",
    "df_test = df_test[:n_test]\n",
    "\n",
    "X_ref = df_ref.drop(target, axis=1).values\n",
    "X_test = df_test.drop(target, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drift? Yes!\n"
     ]
    }
   ],
   "source": [
    "categories_per_feature = {f: None for f in list(category_map.keys())}\n",
    "cd = TabularDrift(X_ref, p_val=.05, categories_per_feature=categories_per_feature, n_features=10) # TODO: Check what is n_features\n",
    "preds = cd.predict(X_test)\n",
    "labels = ['No!', 'Yes!']\n",
    "print('Drift? {}'.format(labels[preds['data']['is_drift']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month -- Chi2 0.000 -- p-value 1.000\n",
      "town -- Chi2 1996.002 -- p-value 0.000\n",
      "flat_model_type -- Chi2 667.474 -- p-value 0.000\n",
      "storey_range -- Chi2 1750.200 -- p-value 0.004\n",
      "dist_to_nearest_stn -- K-S 0.251 -- p-value 0.000\n",
      "dist_to_dhoby -- K-S 0.055 -- p-value 0.094\n",
      "degree_centrality -- K-S 0.218 -- p-value 0.000\n",
      "eigenvector_centrality -- K-S 0.029 -- p-value 0.783\n",
      "remaining_lease_years -- K-S 0.195 -- p-value 0.000\n",
      "floor_area_sqm -- K-S 0.080 -- p-value 0.003\n"
     ]
    }
   ],
   "source": [
    "feature_names = categorical_cols + continuous_cols\n",
    "for f in range(cd.n_features):\n",
    "    stat = 'Chi2' if f in list(categories_per_feature.keys()) else 'K-S'\n",
    "    fname = feature_names[f]\n",
    "    stat_val, p_val = preds['data']['distance'][f], preds['data']['p_val'][f]\n",
    "    print(f'{fname} -- {stat} {stat_val:.3f} -- p-value {p_val:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assuming that the flurry of housing measures have made an impact on the relationship between all the features and resale_price (i.e. P(Y|X) changes), which type of data distribution shift possibly led to model degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From your analysis via TabularDrift, which features contribute to this shift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suggest 1 way to address model degradation and implement it, showing improved test R2 for year 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOG8ZhA98h3O6fnefkjOU9w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
